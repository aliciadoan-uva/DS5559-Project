{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Admission Prediction from NHAMCS\n",
    "## Initial Random Forest model\n",
    "### DS5559: Big Data Analysis\n",
    "### Thomas Hartka, Alicia Doan, Michael Langmayr\n",
    "Created: 7/18/2020 \n",
    "  \n",
    "In this notebook creates and analyzes random forest model for predicting hospital admission in the NHAMCS database.  For this initial model, categorical variables will be represented with one-hot encoding and the reason for visit (RFV) variables will be ignored.  The RFV variables will be ignored because there are hundreds of different potential values and we have not yet developed a way to categorize them yet.  Binary variables were previously converted to 0/1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preferences\n",
    "scale_data = True        # should data be scaled\n",
    "weight_outcome = True    # use weights to handle class imbalance\n",
    "reg_param = 0            # regularization of LR (0=not regularizatoin)\n",
    "elas_param = 0           # elastic net (0=Ridge,1=Lasso)\n",
    "reduced_vars = False     # where to use a reduced variable \n",
    "SEED = 314               # seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data directory\n",
    "data_dir = \"../data\"\n",
    "results_dir = \"../results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/qumulo/qhome/ad2ew/ds5559/DS5559-Project/code'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NHAMCS = spark.read.parquet(data_dir + \"/NHAMCS_processed.2007-2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform string indexing to prepare for OHE for residence variable\n",
    "rsi = StringIndexer(inputCol=\"RESIDNCE\", outputCol=\"RESINDEX\")\n",
    "simodel = rsi.fit(NHAMCS)\n",
    "NHAMCS = simodel.transform(NHAMCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform OHE on residence variable\n",
    "rohe = OneHotEncoder(inputCol='RESINDEX', outputCol='RESONE')\n",
    "NHAMCS = rohe.transform(NHAMCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble vector\n",
    "if reduced_vars:\n",
    "    va = VectorAssembler(inputCols=['AGEYEAR','PULSE','TEMPF','COPD'], \n",
    "                         outputCol=\"features\") \n",
    "else:    \n",
    "    va = VectorAssembler(inputCols=[\"AGEYEAR\",\"RESONE\",'SEXMALE','ARRTIMEMIN','YEAR','PULSE','TEMPF', \\\n",
    "                                'RESPR','BPSYS','BPDIAS','POPCT','PAINSCALE','ALZHD','ASTHMA','CAD','CANCER', \\\n",
    "                                'CEBVD','CHF','CKD','COPD','DEPRN','DIABTYP0','DIABTYP1','DIABTYP2','EDHIV', \\\n",
    "                                'ESRD','ETOHAB','HPE','HTN','HYPLIPID','OBESITY','OSA','OSTPRSIS','SUBSTAB', \\\n",
    "                                'NOCHRON','TOTCHRON','INJURY','INJURY72'], \n",
    "                         outputCol=\"features\")  \n",
    "    \n",
    "NHAMCS = va.setHandleInvalid(\"skip\").transform(NHAMCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features using MaxAbs\n",
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(NHAMCS)\n",
    "NHAMCS = scalerModel.transform(NHAMCS)\n",
    "\n",
    "# determine feature column\n",
    "if scale_data:\n",
    "    features_col = \"features\"\n",
    "else:\n",
    "    features_col = \"scaledFeatures\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Data by Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, target, positive_label, negative_label):\n",
    "    \"\"\"\n",
    "    df              spark dataframe\n",
    "    target          str, target variable\n",
    "    positive_label  int, value of positive label\n",
    "    negative_label  int, value of negative label\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ### ENTER CODE HERE\n",
    "    positives = df.filter(df[target] == positive_label)\n",
    "    num_positives = positives.count()\n",
    "    negatives =  df.filter(df[target] == negative_label)\n",
    "    num_negatives = negatives.count()\n",
    "    \n",
    "    if (num_positives > num_negatives): # downsample positives\n",
    "        sampled_df = positives.sample(withReplacement=False, fraction=num_negatives/num_positives, seed=SEED)\n",
    "        df_b = sampled_df.union(negatives)\n",
    "    elif (num_negatives > num_positives): # downsample negatives\n",
    "        sampled_df = negatives.sample(withReplacement=False, fraction=num_positives/num_negatives, seed=SEED)\n",
    "        df_b = sampled_df.union(positives)\n",
    "\n",
    "    return df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "779"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NHAMCS_small = downsample(NHAMCS, 'ADM_OUTCOME', 1, 0)\n",
    "NHAMCS_small.filter(NHAMCS_small['ADM_OUTCOME'] == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRDD = NHAMCS.select(col(\"ADM_OUTCOME\"), col(\"scaledFeatures\")).rdd.map(tuple)\n",
    "dataRDD_small = NHAMCS_small.select(col(\"ADM_OUTCOME\"), col(\"scaledFeatures\")).rdd.map(tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map label to binary values, then convert to LabeledPoint\n",
    "lp = dataRDD.map(lambda row:(row[0], Vectors.dense(row[1])))    \\\n",
    "                    .map(lambda row: LabeledPoint(row[0], row[1]))\n",
    "\n",
    "lp_small = dataRDD_small.map(lambda row:(row[0], Vectors.dense(row[1])))    \\\n",
    "                    .map(lambda row: LabeledPoint(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1:2, 2:2, 3:2, 4:2, 5:2, 6:2, 7:2, 17:2, 18:2, 19:2, 20:2, 21:2, 22:2, 23:2, 24:2, 25:2, 26:2, 27:2, 28:2, 29:2, 30:2, 31:2, 32:2, 33:2, 34:2, 35:2, 36:2, 37:2, 38:2, 39:2, 40:2, 41:2, 42:2]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#found which features were categorical -- plug into categoricalFeaturesInfo in model\n",
    "# remove 40th feature\n",
    "dense_vector = [37.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,824.0,2014.0,85.0,98.0,16.0,123.0,67.0,100.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,1.0]\n",
    "categorical_list = [str(i)+ \":2\" for i in range(len(dense_vector)) if (int(dense_vector[i])== 0) |(int(dense_vector[i])== 1)] \n",
    "\n",
    "str(categorical_list).replace(\"'\",\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing set\n",
    "training, testing = lp.randomSplit([0.8, 0.2], SEED) #using non-downsampled data -- 93% accuracy\n",
    "#training, testing = lp_small.randomSplit([0.8, 0.2], SEED) #using downsampled data -- got about 70% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categoricalFeaturesInfo={1:2, 2:2, 3:2, 4:2, 5:2, 6:2, 7:2, 17:2, 18:2, 19:2, 20:2, 21:2, 22:2, 23:2, 24:2, 25:2, 26:2, 27:2, 28:2, 29:2, 30:2, 31:2, 32:2, 33:2, 34:2, 35:2, 36:2, 37:2, 38:2, 39:2, 41:2, 42:2}\n",
    "\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(training, numClasses=2, categoricalFeaturesInfo=categoricalFeaturesInfo,\n",
    "                                     numTrees=200, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=15, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.07025344597598933\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testing.map(lambda x: x.features))\n",
    "labelsAndPredictions = testing.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testing.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[138.  54.]\n",
      " [ 29.  97.]]\n"
     ]
    }
   ],
   "source": [
    "confusion_metrics = MulticlassMetrics(labelsAndPredictions)\n",
    "print('confusion matrix {}'.format(confusion_metrics.confusionMatrix().toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "\tLabel:\n",
      "Pred\t 0:\t 1:\n",
      "0:\t 2090 \t 158\n",
      "1:\t 0 \t 1\n",
      "\n",
      "Confusion Matrix:\n",
      "tn: 2090  fn: 158\n",
      "fp: 0   tp: 1\n"
     ]
    }
   ],
   "source": [
    "def confusion(labelsAndPreds):\n",
    "    # Additionally, compute and show the confusion matrix.\n",
    "    tp = labelsAndPreds.filter(lambda pl: (pl[0] == 1.0) & (pl[1] == 1.0)).count()\n",
    "    fp = labelsAndPreds.filter(lambda pl: (pl[0] == 0.0) & (pl[1] == 1.0)).count()\n",
    "    tn = labelsAndPreds.filter(lambda pl: (pl[0] == 0.0) & (pl[1] == 0.0)).count()\n",
    "    fn = labelsAndPreds.filter(lambda pl: (pl[0] == 1.0) & (pl[1] == 0.0)).count()\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"\\tLabel:\")\n",
    "    print(\"Pred\\t 0:\\t 1:\")\n",
    "    print('0:\\t',tn,'\\t',fn)\n",
    "    print('1:\\t',fp, '\\t',tp,)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print('tn:',tn,' fn:',fn)\n",
    "    print('fp:',fp, '  tp:',tp,)\n",
    "\n",
    "\n",
    "confusion(labelsAndPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"ADM_OUTCOME\")\n",
    "\n",
    "# print(\"Number of negatives: \", predict_test.where('prediction == 0').count())\n",
    "# print(\"Number of positives: \", predict_test.where('prediction == 1').count())\n",
    "\n",
    "# print(\"\\nThe area under ROC for train set is\", evaluator.evaluate(predict_train))\n",
    "# print(\"The area under ROC for test set is\", evaluator.evaluate(predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under Precision/Recall (PR) curve: 1\n",
      "Area under Receiver Operating Characteristic (ROC) curve: 96.486\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "\n",
    "metrics = BinaryClassificationMetrics(labelsAndPredictions)\n",
    "print(\"Area under Precision/Recall (PR) curve: %.f\" % (metrics.areaUnderPR * 100))\n",
    "print(\"Area under Receiver Operating Characteristic (ROC) curve: %.3f\" % (metrics.areaUnderROC * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
